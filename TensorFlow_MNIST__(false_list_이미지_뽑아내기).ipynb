{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_MNIST_ (false list 이미지 뽑아내기).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPexLotfwEtNCaIj3MYnB83",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hongjieun123/deeplearning-project/blob/master/TensorFlow_MNIST__(false_list_%EC%9D%B4%EB%AF%B8%EC%A7%80_%EB%BD%91%EC%95%84%EB%82%B4%EA%B8%B0).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln5b0cAI-HNa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5a76fd10-2aab-44e0-930b-1cb0249b825c"
      },
      "source": [
        "# Colab에서 tensorflow 1.x 실행\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0kspSnE9Xa9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "0456c455-9120-423a-b2ee-5880b92338f9"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
        "\n",
        "print('')\n",
        "print('train num = ', mnist.train.num_examples, ', test num = ', mnist.test.num_examples, ', validation num = ', mnist.validation.num_examples)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-60cca8ee44f0>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "\n",
            "train num =  55000 , test num =  10000 , validation num =  5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztz6mHQtGslo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "c3e6dd2a-dd98-4ae0-e619-4b95effe923c"
      },
      "source": [
        "print('type(mnist) = ', type(mnist), ' ,type(mnist.train.images) = ', type(mnist.train.images), ', type(mnist.train.labels) = ', type(mnist.train.labels))\n",
        "\n",
        "print('\\ntrain image shape = ', np.shape(mnist.train.images))\n",
        "print('train label shape = ', np.shape(mnist.train.labels))\n",
        "print('test image shape = ', np.shape(mnist.test.images))\n",
        "\n",
        "print('\\ntrain image shape = ', mnist.train.images.shape)\n",
        "print('test image shape = ', mnist.test.images.shape)\n",
        "print('validation image shape = ', mnist.validation.images.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type(mnist) =  <class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>  ,type(mnist.train.images) =  <class 'numpy.ndarray'> , type(mnist.train.labels) =  <class 'numpy.ndarray'>\n",
            "\n",
            "train image shape =  (55000, 784)\n",
            "train label shape =  (55000, 10)\n",
            "test image shape =  (10000, 784)\n",
            "\n",
            "train image shape =  (55000, 784)\n",
            "test image shape =  (10000, 784)\n",
            "validation image shape =  (5000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYHX0PBrGsju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "44c4f130-521f-4ec0-84bd-12590eb2f9c0"
      },
      "source": [
        "print('len(mnist.train.images[0]) = ', len(mnist.train.images[0]))\n",
        "print(mnist.train.images[0])\n",
        "print('One-Hot Encoding 확인\\n')\n",
        "print('len(mnist.train.labels[0]) = ', len(mnist.train.labels[0]))\n",
        "print(mnist.train.labels[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(mnist.train.images[0]) =  784\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
            " 0.46274513 0.2392157  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.3529412\n",
            " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
            " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
            " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
            " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
            " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
            " 0.7411765  0.09019608 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
            " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
            " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
            " 0.08235294 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.14901961 0.32156864\n",
            " 0.0509804  0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.32941177\n",
            " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
            " 0.9176471  0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.09803922\n",
            " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
            " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
            " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
            " 0.9960785  0.5568628  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
            " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
            " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
            " 0.34901962 0.12156864 0.         0.         0.         0.\n",
            " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.6627451  0.9960785\n",
            " 0.6901961  0.24313727 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
            " 0.9176471  0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.07058824 0.48627454 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.54509807\n",
            " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
            " 0.3372549  0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.01568628 0.45882356\n",
            " 0.27058825 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n",
            "One-Hot Encoding 확인\n",
            "\n",
            "len(mnist.train.labels[0]) =  10\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am8xeb1fGsgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "batch_size = 100\n",
        "\n",
        "input_nodes = 784\n",
        "hidden_nodes = 10\n",
        "output_nodes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL2OcaLVGseC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.placeholder(tf.float32, [None, input_nodes]) # (none, ) == reshape(-1,)\n",
        "T = tf.placeholder(tf.float32, [None, output_nodes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaIRAqa-GscO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W2 = tf.Variable(tf.random_normal([input_nodes, hidden_nodes]))\n",
        "b2 = tf.Variable(tf.random_normal([hidden_nodes]))\n",
        "W3 = tf.Variable(tf.random_normal([hidden_nodes,output_nodes]))\n",
        "b3 = tf.Variable(tf.random_normal([output_nodes]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFM5narWGsZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Z2 = tf.matmul(X,W2) + b2\n",
        "A2 = tf.nn.relu(Z2)\n",
        "\n",
        "Z3 = logits = tf.matmul(A2,W3) + b3   # softmax의 입력값은 logits\n",
        "\n",
        "y = A3 = tf.nn.softmax(Z3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7NMtyA0GsW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3, labels=T))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "train = optimizer.minimize(loss) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HyN9tsbGsUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_val = tf.equal(tf.argmax(A3,1), tf.argmax(T,1)) #같으면 true, 다르면 false\n",
        "accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32)) # tf.case = true면 1 false면 0으로 바꿔줌\n",
        "accuracy_index = tf.cast(predicted_val, dtype=tf.float32)\n",
        "predicted_list = tf.argmax(A3,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJtmJu4mGsR8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "27722914-6c72-4ad8-a2cb-1f3b879385ef"
      },
      "source": [
        "#세션을 실행해야지만 이 값이 실행된다.\n",
        "print('type(predicted_val) = ', type(predicted_val),  ', type(accuracy) = ', type(accuracy))\n",
        "print('type(accuracy_index) =', type(accuracy_index), ', type(predicted_list) = ', type(predicted_list))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type(predicted_val) =  <class 'tensorflow.python.framework.ops.Tensor'> , type(accuracy) =  <class 'tensorflow.python.framework.ops.Tensor'>\n",
            "type(accuracy_index) = <class 'tensorflow.python.framework.ops.Tensor'> , type(predicted_list) =  <class 'tensorflow.python.framework.ops.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxSyqPyIGsP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03276f50-06d1-4c43-e62c-3c5a2b9bd803"
      },
      "source": [
        "index_label_prediction_list = []\n",
        "\n",
        "with  tf.Session()  as sess:\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())  \n",
        "        \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    for i in range(epochs):    \n",
        "        \n",
        "        total_batch = int(mnist.train.num_examples / batch_size)  # 55,000 / 100\n",
        "\n",
        "        for step in range(total_batch):\n",
        "            \n",
        "            batch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n",
        "      \n",
        "            loss_val, _ = sess.run([loss, train], feed_dict={X: batch_x_data, T: batch_t_data})    \n",
        "        \n",
        "            if step % 100 == 0:\n",
        "                print(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)             \n",
        "    \n",
        "    end_time = datetime.now()\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"Elapsed Time => \", end_time-start_time)\n",
        "    print(\"\")\n",
        "    \n",
        "    # Accuracy 확인\n",
        "    test_x_data = mnist.test.images   \n",
        "    test_t_data = mnist.test.labels   \n",
        "    \n",
        "    accuracy_val, predicted_list_val, index_label = sess.run([accuracy, predicted_list, accuracy_index], feed_dict={X: test_x_data, T: test_t_data})\n",
        "    \n",
        "    print(\"\\nAccuracy = \", accuracy_val)\n",
        "    print(\"type(accuracy_val) = \", type(accuracy_val), ', type(predicted_list_val) = ', type(predicted_list_val), ', type(index_label) = ', type(index_label))\n",
        "    print(\"index_label.shape = \", index_label.shape)\n",
        "    \n",
        "    index_label_list = list(index_label)\n",
        "    print(\"length of index_label_list = \", len(index_label_list))\n",
        "    print(\"false label count = \", index_label_list.count([0]))\n",
        "    \n",
        "    \n",
        "    # list type 으로 디버그\n",
        "    temp_list = [] \n",
        "    \n",
        "    for index in range(len(index_label_list)):\n",
        "        \n",
        "        if index_label_list[index] == 0:\n",
        "            \n",
        "            temp_list.append(index)\n",
        "            temp_list.append(np.argmax(test_t_data[index]))  # one-hot encoding 이므로 argmax 로 정답 추출\n",
        "            temp_list.append(predicted_list_val[index])\n",
        "            \n",
        "            index_label_prediction_list.append(temp_list)\n",
        "            \n",
        "            temp_list = []\n",
        "            \n",
        "    print(\"\\nlength of index_label_prediction_list\", len(index_label_prediction_list))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs =  0 , step =  0 , loss_val =  19.545195\n",
            "epochs =  0 , step =  100 , loss_val =  2.264453\n",
            "epochs =  0 , step =  200 , loss_val =  2.1971471\n",
            "epochs =  0 , step =  300 , loss_val =  1.9959179\n",
            "epochs =  0 , step =  400 , loss_val =  1.7824445\n",
            "epochs =  0 , step =  500 , loss_val =  1.6887267\n",
            "epochs =  1 , step =  0 , loss_val =  1.875329\n",
            "epochs =  1 , step =  100 , loss_val =  1.6358974\n",
            "epochs =  1 , step =  200 , loss_val =  1.3768314\n",
            "epochs =  1 , step =  300 , loss_val =  1.3776623\n",
            "epochs =  1 , step =  400 , loss_val =  1.2767386\n",
            "epochs =  1 , step =  500 , loss_val =  1.13699\n",
            "epochs =  2 , step =  0 , loss_val =  1.1773741\n",
            "epochs =  2 , step =  100 , loss_val =  1.2442825\n",
            "epochs =  2 , step =  200 , loss_val =  1.277112\n",
            "epochs =  2 , step =  300 , loss_val =  0.8383291\n",
            "epochs =  2 , step =  400 , loss_val =  0.9179571\n",
            "epochs =  2 , step =  500 , loss_val =  1.0805995\n",
            "epochs =  3 , step =  0 , loss_val =  1.117387\n",
            "epochs =  3 , step =  100 , loss_val =  1.1783195\n",
            "epochs =  3 , step =  200 , loss_val =  1.0426646\n",
            "epochs =  3 , step =  300 , loss_val =  1.0580829\n",
            "epochs =  3 , step =  400 , loss_val =  1.0482997\n",
            "epochs =  3 , step =  500 , loss_val =  1.3028624\n",
            "epochs =  4 , step =  0 , loss_val =  1.0892177\n",
            "epochs =  4 , step =  100 , loss_val =  0.8568541\n",
            "epochs =  4 , step =  200 , loss_val =  1.3495514\n",
            "epochs =  4 , step =  300 , loss_val =  0.9619775\n",
            "epochs =  4 , step =  400 , loss_val =  1.1294646\n",
            "epochs =  4 , step =  500 , loss_val =  1.0954659\n",
            "epochs =  5 , step =  0 , loss_val =  0.9389831\n",
            "epochs =  5 , step =  100 , loss_val =  1.0692074\n",
            "epochs =  5 , step =  200 , loss_val =  0.92374015\n",
            "epochs =  5 , step =  300 , loss_val =  1.1759765\n",
            "epochs =  5 , step =  400 , loss_val =  1.1621907\n",
            "epochs =  5 , step =  500 , loss_val =  1.0726428\n",
            "epochs =  6 , step =  0 , loss_val =  1.1823453\n",
            "epochs =  6 , step =  100 , loss_val =  0.77496314\n",
            "epochs =  6 , step =  200 , loss_val =  0.97715706\n",
            "epochs =  6 , step =  300 , loss_val =  0.9867104\n",
            "epochs =  6 , step =  400 , loss_val =  0.87725043\n",
            "epochs =  6 , step =  500 , loss_val =  1.1631792\n",
            "epochs =  7 , step =  0 , loss_val =  0.8853613\n",
            "epochs =  7 , step =  100 , loss_val =  0.6487287\n",
            "epochs =  7 , step =  200 , loss_val =  1.0690573\n",
            "epochs =  7 , step =  300 , loss_val =  0.8490194\n",
            "epochs =  7 , step =  400 , loss_val =  0.676847\n",
            "epochs =  7 , step =  500 , loss_val =  0.99060255\n",
            "epochs =  8 , step =  0 , loss_val =  0.8951835\n",
            "epochs =  8 , step =  100 , loss_val =  0.84872115\n",
            "epochs =  8 , step =  200 , loss_val =  0.65740806\n",
            "epochs =  8 , step =  300 , loss_val =  0.82454973\n",
            "epochs =  8 , step =  400 , loss_val =  0.7616139\n",
            "epochs =  8 , step =  500 , loss_val =  0.81405854\n",
            "epochs =  9 , step =  0 , loss_val =  0.80938727\n",
            "epochs =  9 , step =  100 , loss_val =  0.6690273\n",
            "epochs =  9 , step =  200 , loss_val =  0.7642675\n",
            "epochs =  9 , step =  300 , loss_val =  0.7637855\n",
            "epochs =  9 , step =  400 , loss_val =  0.71696717\n",
            "epochs =  9 , step =  500 , loss_val =  0.7095067\n",
            "epochs =  10 , step =  0 , loss_val =  0.7728682\n",
            "epochs =  10 , step =  100 , loss_val =  0.90006614\n",
            "epochs =  10 , step =  200 , loss_val =  0.71110183\n",
            "epochs =  10 , step =  300 , loss_val =  0.60035145\n",
            "epochs =  10 , step =  400 , loss_val =  0.68686765\n",
            "epochs =  10 , step =  500 , loss_val =  0.65982836\n",
            "epochs =  11 , step =  0 , loss_val =  0.7893113\n",
            "epochs =  11 , step =  100 , loss_val =  0.71975386\n",
            "epochs =  11 , step =  200 , loss_val =  0.52436155\n",
            "epochs =  11 , step =  300 , loss_val =  0.67507833\n",
            "epochs =  11 , step =  400 , loss_val =  0.5340513\n",
            "epochs =  11 , step =  500 , loss_val =  0.6013319\n",
            "epochs =  12 , step =  0 , loss_val =  0.5611621\n",
            "epochs =  12 , step =  100 , loss_val =  0.70071787\n",
            "epochs =  12 , step =  200 , loss_val =  0.77374315\n",
            "epochs =  12 , step =  300 , loss_val =  0.61265326\n",
            "epochs =  12 , step =  400 , loss_val =  0.5600545\n",
            "epochs =  12 , step =  500 , loss_val =  0.7046898\n",
            "epochs =  13 , step =  0 , loss_val =  0.39816165\n",
            "epochs =  13 , step =  100 , loss_val =  0.71928453\n",
            "epochs =  13 , step =  200 , loss_val =  0.72866094\n",
            "epochs =  13 , step =  300 , loss_val =  0.75355524\n",
            "epochs =  13 , step =  400 , loss_val =  0.65747565\n",
            "epochs =  13 , step =  500 , loss_val =  0.5996825\n",
            "epochs =  14 , step =  0 , loss_val =  0.878369\n",
            "epochs =  14 , step =  100 , loss_val =  0.823125\n",
            "epochs =  14 , step =  200 , loss_val =  0.5568159\n",
            "epochs =  14 , step =  300 , loss_val =  0.5866467\n",
            "epochs =  14 , step =  400 , loss_val =  0.711795\n",
            "epochs =  14 , step =  500 , loss_val =  0.6046862\n",
            "epochs =  15 , step =  0 , loss_val =  0.5187933\n",
            "epochs =  15 , step =  100 , loss_val =  0.7814269\n",
            "epochs =  15 , step =  200 , loss_val =  0.7514891\n",
            "epochs =  15 , step =  300 , loss_val =  0.74868363\n",
            "epochs =  15 , step =  400 , loss_val =  0.5859992\n",
            "epochs =  15 , step =  500 , loss_val =  0.6136238\n",
            "epochs =  16 , step =  0 , loss_val =  0.7652898\n",
            "epochs =  16 , step =  100 , loss_val =  0.6803272\n",
            "epochs =  16 , step =  200 , loss_val =  0.6606672\n",
            "epochs =  16 , step =  300 , loss_val =  0.78600293\n",
            "epochs =  16 , step =  400 , loss_val =  0.8535131\n",
            "epochs =  16 , step =  500 , loss_val =  0.6777302\n",
            "epochs =  17 , step =  0 , loss_val =  0.5762358\n",
            "epochs =  17 , step =  100 , loss_val =  0.6180751\n",
            "epochs =  17 , step =  200 , loss_val =  0.58784807\n",
            "epochs =  17 , step =  300 , loss_val =  0.6684008\n",
            "epochs =  17 , step =  400 , loss_val =  0.6496315\n",
            "epochs =  17 , step =  500 , loss_val =  0.53304666\n",
            "epochs =  18 , step =  0 , loss_val =  0.7388992\n",
            "epochs =  18 , step =  100 , loss_val =  0.70840424\n",
            "epochs =  18 , step =  200 , loss_val =  0.5397613\n",
            "epochs =  18 , step =  300 , loss_val =  0.4957326\n",
            "epochs =  18 , step =  400 , loss_val =  0.6232824\n",
            "epochs =  18 , step =  500 , loss_val =  0.55074674\n",
            "epochs =  19 , step =  0 , loss_val =  0.5340727\n",
            "epochs =  19 , step =  100 , loss_val =  0.5225647\n",
            "epochs =  19 , step =  200 , loss_val =  0.49586788\n",
            "epochs =  19 , step =  300 , loss_val =  0.72890097\n",
            "epochs =  19 , step =  400 , loss_val =  0.61131537\n",
            "epochs =  19 , step =  500 , loss_val =  0.53986907\n",
            "epochs =  20 , step =  0 , loss_val =  0.51740646\n",
            "epochs =  20 , step =  100 , loss_val =  0.7012587\n",
            "epochs =  20 , step =  200 , loss_val =  0.38246095\n",
            "epochs =  20 , step =  300 , loss_val =  0.709634\n",
            "epochs =  20 , step =  400 , loss_val =  0.57193846\n",
            "epochs =  20 , step =  500 , loss_val =  0.5183207\n",
            "epochs =  21 , step =  0 , loss_val =  0.7516594\n",
            "epochs =  21 , step =  100 , loss_val =  0.6716131\n",
            "epochs =  21 , step =  200 , loss_val =  0.460354\n",
            "epochs =  21 , step =  300 , loss_val =  0.35680044\n",
            "epochs =  21 , step =  400 , loss_val =  0.6695177\n",
            "epochs =  21 , step =  500 , loss_val =  0.47984874\n",
            "epochs =  22 , step =  0 , loss_val =  0.7101587\n",
            "epochs =  22 , step =  100 , loss_val =  0.7323307\n",
            "epochs =  22 , step =  200 , loss_val =  0.62419695\n",
            "epochs =  22 , step =  300 , loss_val =  0.6516807\n",
            "epochs =  22 , step =  400 , loss_val =  0.59518903\n",
            "epochs =  22 , step =  500 , loss_val =  0.5994815\n",
            "epochs =  23 , step =  0 , loss_val =  0.57252127\n",
            "epochs =  23 , step =  100 , loss_val =  0.5428456\n",
            "epochs =  23 , step =  200 , loss_val =  0.6293417\n",
            "epochs =  23 , step =  300 , loss_val =  0.54659414\n",
            "epochs =  23 , step =  400 , loss_val =  0.6190582\n",
            "epochs =  23 , step =  500 , loss_val =  0.5488766\n",
            "epochs =  24 , step =  0 , loss_val =  0.605462\n",
            "epochs =  24 , step =  100 , loss_val =  0.8041301\n",
            "epochs =  24 , step =  200 , loss_val =  0.52451473\n",
            "epochs =  24 , step =  300 , loss_val =  0.46835685\n",
            "epochs =  24 , step =  400 , loss_val =  0.45985198\n",
            "epochs =  24 , step =  500 , loss_val =  0.33944416\n",
            "epochs =  25 , step =  0 , loss_val =  0.45519257\n",
            "epochs =  25 , step =  100 , loss_val =  0.5377683\n",
            "epochs =  25 , step =  200 , loss_val =  0.46298602\n",
            "epochs =  25 , step =  300 , loss_val =  0.5412615\n",
            "epochs =  25 , step =  400 , loss_val =  0.51279616\n",
            "epochs =  25 , step =  500 , loss_val =  0.48202348\n",
            "epochs =  26 , step =  0 , loss_val =  0.40097725\n",
            "epochs =  26 , step =  100 , loss_val =  0.5810106\n",
            "epochs =  26 , step =  200 , loss_val =  0.39032036\n",
            "epochs =  26 , step =  300 , loss_val =  0.39813396\n",
            "epochs =  26 , step =  400 , loss_val =  0.57505184\n",
            "epochs =  26 , step =  500 , loss_val =  0.68856674\n",
            "epochs =  27 , step =  0 , loss_val =  0.4137895\n",
            "epochs =  27 , step =  100 , loss_val =  0.5277462\n",
            "epochs =  27 , step =  200 , loss_val =  0.39246953\n",
            "epochs =  27 , step =  300 , loss_val =  0.51152086\n",
            "epochs =  27 , step =  400 , loss_val =  0.49850997\n",
            "epochs =  27 , step =  500 , loss_val =  0.4233889\n",
            "epochs =  28 , step =  0 , loss_val =  0.45614204\n",
            "epochs =  28 , step =  100 , loss_val =  0.52635646\n",
            "epochs =  28 , step =  200 , loss_val =  0.4544197\n",
            "epochs =  28 , step =  300 , loss_val =  0.46118897\n",
            "epochs =  28 , step =  400 , loss_val =  0.50657636\n",
            "epochs =  28 , step =  500 , loss_val =  0.448646\n",
            "epochs =  29 , step =  0 , loss_val =  0.33029163\n",
            "epochs =  29 , step =  100 , loss_val =  0.5319261\n",
            "epochs =  29 , step =  200 , loss_val =  0.5821399\n",
            "epochs =  29 , step =  300 , loss_val =  0.49608317\n",
            "epochs =  29 , step =  400 , loss_val =  0.5444401\n",
            "epochs =  29 , step =  500 , loss_val =  0.5664603\n",
            "epochs =  30 , step =  0 , loss_val =  0.34241858\n",
            "epochs =  30 , step =  100 , loss_val =  0.3490989\n",
            "epochs =  30 , step =  200 , loss_val =  0.5159424\n",
            "epochs =  30 , step =  300 , loss_val =  0.37045407\n",
            "epochs =  30 , step =  400 , loss_val =  0.32987037\n",
            "epochs =  30 , step =  500 , loss_val =  0.3578954\n",
            "epochs =  31 , step =  0 , loss_val =  0.505415\n",
            "epochs =  31 , step =  100 , loss_val =  0.40097836\n",
            "epochs =  31 , step =  200 , loss_val =  0.48392716\n",
            "epochs =  31 , step =  300 , loss_val =  0.39338475\n",
            "epochs =  31 , step =  400 , loss_val =  0.5381498\n",
            "epochs =  31 , step =  500 , loss_val =  0.45541504\n",
            "epochs =  32 , step =  0 , loss_val =  0.3255926\n",
            "epochs =  32 , step =  100 , loss_val =  0.364849\n",
            "epochs =  32 , step =  200 , loss_val =  0.46138328\n",
            "epochs =  32 , step =  300 , loss_val =  0.5845866\n",
            "epochs =  32 , step =  400 , loss_val =  0.6156106\n",
            "epochs =  32 , step =  500 , loss_val =  0.5351396\n",
            "epochs =  33 , step =  0 , loss_val =  0.55169266\n",
            "epochs =  33 , step =  100 , loss_val =  0.6579807\n",
            "epochs =  33 , step =  200 , loss_val =  0.23462868\n",
            "epochs =  33 , step =  300 , loss_val =  0.3489572\n",
            "epochs =  33 , step =  400 , loss_val =  0.43133366\n",
            "epochs =  33 , step =  500 , loss_val =  0.4104351\n",
            "epochs =  34 , step =  0 , loss_val =  0.475393\n",
            "epochs =  34 , step =  100 , loss_val =  0.4576506\n",
            "epochs =  34 , step =  200 , loss_val =  0.44598985\n",
            "epochs =  34 , step =  300 , loss_val =  0.2343797\n",
            "epochs =  34 , step =  400 , loss_val =  0.3942915\n",
            "epochs =  34 , step =  500 , loss_val =  0.3373868\n",
            "epochs =  35 , step =  0 , loss_val =  0.5634331\n",
            "epochs =  35 , step =  100 , loss_val =  0.3371891\n",
            "epochs =  35 , step =  200 , loss_val =  0.3257703\n",
            "epochs =  35 , step =  300 , loss_val =  0.74335974\n",
            "epochs =  35 , step =  400 , loss_val =  0.51829183\n",
            "epochs =  35 , step =  500 , loss_val =  0.63542\n",
            "epochs =  36 , step =  0 , loss_val =  0.31330663\n",
            "epochs =  36 , step =  100 , loss_val =  0.38299343\n",
            "epochs =  36 , step =  200 , loss_val =  0.4917177\n",
            "epochs =  36 , step =  300 , loss_val =  0.45098364\n",
            "epochs =  36 , step =  400 , loss_val =  0.60992986\n",
            "epochs =  36 , step =  500 , loss_val =  0.49603218\n",
            "epochs =  37 , step =  0 , loss_val =  0.35525176\n",
            "epochs =  37 , step =  100 , loss_val =  0.5069821\n",
            "epochs =  37 , step =  200 , loss_val =  0.30431974\n",
            "epochs =  37 , step =  300 , loss_val =  0.40662524\n",
            "epochs =  37 , step =  400 , loss_val =  0.4605502\n",
            "epochs =  37 , step =  500 , loss_val =  0.5921182\n",
            "epochs =  38 , step =  0 , loss_val =  0.4347811\n",
            "epochs =  38 , step =  100 , loss_val =  0.19811642\n",
            "epochs =  38 , step =  200 , loss_val =  0.5934444\n",
            "epochs =  38 , step =  300 , loss_val =  0.30939597\n",
            "epochs =  38 , step =  400 , loss_val =  0.3623926\n",
            "epochs =  38 , step =  500 , loss_val =  0.27061114\n",
            "epochs =  39 , step =  0 , loss_val =  0.29805472\n",
            "epochs =  39 , step =  100 , loss_val =  0.53851736\n",
            "epochs =  39 , step =  200 , loss_val =  0.29878348\n",
            "epochs =  39 , step =  300 , loss_val =  0.3777439\n",
            "epochs =  39 , step =  400 , loss_val =  0.5197696\n",
            "epochs =  39 , step =  500 , loss_val =  0.25269294\n",
            "epochs =  40 , step =  0 , loss_val =  0.5617306\n",
            "epochs =  40 , step =  100 , loss_val =  0.55483514\n",
            "epochs =  40 , step =  200 , loss_val =  0.48924062\n",
            "epochs =  40 , step =  300 , loss_val =  0.47898206\n",
            "epochs =  40 , step =  400 , loss_val =  0.39264748\n",
            "epochs =  40 , step =  500 , loss_val =  0.49178392\n",
            "epochs =  41 , step =  0 , loss_val =  0.45389175\n",
            "epochs =  41 , step =  100 , loss_val =  0.3696909\n",
            "epochs =  41 , step =  200 , loss_val =  0.49560654\n",
            "epochs =  41 , step =  300 , loss_val =  0.26751202\n",
            "epochs =  41 , step =  400 , loss_val =  0.40835956\n",
            "epochs =  41 , step =  500 , loss_val =  0.42724925\n",
            "epochs =  42 , step =  0 , loss_val =  0.32054004\n",
            "epochs =  42 , step =  100 , loss_val =  0.49486336\n",
            "epochs =  42 , step =  200 , loss_val =  0.55877036\n",
            "epochs =  42 , step =  300 , loss_val =  0.59560984\n",
            "epochs =  42 , step =  400 , loss_val =  0.4120703\n",
            "epochs =  42 , step =  500 , loss_val =  0.3518153\n",
            "epochs =  43 , step =  0 , loss_val =  0.58354735\n",
            "epochs =  43 , step =  100 , loss_val =  0.39057338\n",
            "epochs =  43 , step =  200 , loss_val =  0.39248684\n",
            "epochs =  43 , step =  300 , loss_val =  0.4584623\n",
            "epochs =  43 , step =  400 , loss_val =  0.41577077\n",
            "epochs =  43 , step =  500 , loss_val =  0.34589332\n",
            "epochs =  44 , step =  0 , loss_val =  0.39601612\n",
            "epochs =  44 , step =  100 , loss_val =  0.4023187\n",
            "epochs =  44 , step =  200 , loss_val =  0.24035016\n",
            "epochs =  44 , step =  300 , loss_val =  0.4897129\n",
            "epochs =  44 , step =  400 , loss_val =  0.50540197\n",
            "epochs =  44 , step =  500 , loss_val =  0.23616728\n",
            "epochs =  45 , step =  0 , loss_val =  0.8375067\n",
            "epochs =  45 , step =  100 , loss_val =  0.37833428\n",
            "epochs =  45 , step =  200 , loss_val =  0.4086516\n",
            "epochs =  45 , step =  300 , loss_val =  0.51276565\n",
            "epochs =  45 , step =  400 , loss_val =  0.3891826\n",
            "epochs =  45 , step =  500 , loss_val =  0.44006634\n",
            "epochs =  46 , step =  0 , loss_val =  0.59069467\n",
            "epochs =  46 , step =  100 , loss_val =  0.28610078\n",
            "epochs =  46 , step =  200 , loss_val =  0.45427898\n",
            "epochs =  46 , step =  300 , loss_val =  0.18962568\n",
            "epochs =  46 , step =  400 , loss_val =  0.6411792\n",
            "epochs =  46 , step =  500 , loss_val =  0.3830945\n",
            "epochs =  47 , step =  0 , loss_val =  0.30674514\n",
            "epochs =  47 , step =  100 , loss_val =  0.57472813\n",
            "epochs =  47 , step =  200 , loss_val =  0.3095129\n",
            "epochs =  47 , step =  300 , loss_val =  0.40966117\n",
            "epochs =  47 , step =  400 , loss_val =  0.32766864\n",
            "epochs =  47 , step =  500 , loss_val =  0.29089686\n",
            "epochs =  48 , step =  0 , loss_val =  0.25554618\n",
            "epochs =  48 , step =  100 , loss_val =  0.44421473\n",
            "epochs =  48 , step =  200 , loss_val =  0.21293788\n",
            "epochs =  48 , step =  300 , loss_val =  0.41056868\n",
            "epochs =  48 , step =  400 , loss_val =  0.25732324\n",
            "epochs =  48 , step =  500 , loss_val =  0.16792773\n",
            "epochs =  49 , step =  0 , loss_val =  0.3169523\n",
            "epochs =  49 , step =  100 , loss_val =  0.45162174\n",
            "epochs =  49 , step =  200 , loss_val =  0.3601439\n",
            "epochs =  49 , step =  300 , loss_val =  0.38321158\n",
            "epochs =  49 , step =  400 , loss_val =  0.4575765\n",
            "epochs =  49 , step =  500 , loss_val =  0.44712737\n",
            "epochs =  50 , step =  0 , loss_val =  0.4330128\n",
            "epochs =  50 , step =  100 , loss_val =  0.44077605\n",
            "epochs =  50 , step =  200 , loss_val =  0.34491885\n",
            "epochs =  50 , step =  300 , loss_val =  0.2372593\n",
            "epochs =  50 , step =  400 , loss_val =  0.47061455\n",
            "epochs =  50 , step =  500 , loss_val =  0.37613288\n",
            "epochs =  51 , step =  0 , loss_val =  0.2250536\n",
            "epochs =  51 , step =  100 , loss_val =  0.41627255\n",
            "epochs =  51 , step =  200 , loss_val =  0.5184922\n",
            "epochs =  51 , step =  300 , loss_val =  0.34081158\n",
            "epochs =  51 , step =  400 , loss_val =  0.37810022\n",
            "epochs =  51 , step =  500 , loss_val =  0.53055346\n",
            "epochs =  52 , step =  0 , loss_val =  0.38961354\n",
            "epochs =  52 , step =  100 , loss_val =  0.3392725\n",
            "epochs =  52 , step =  200 , loss_val =  0.5538527\n",
            "epochs =  52 , step =  300 , loss_val =  0.31794348\n",
            "epochs =  52 , step =  400 , loss_val =  0.39314649\n",
            "epochs =  52 , step =  500 , loss_val =  0.34365255\n",
            "epochs =  53 , step =  0 , loss_val =  0.5960021\n",
            "epochs =  53 , step =  100 , loss_val =  0.2737459\n",
            "epochs =  53 , step =  200 , loss_val =  0.40264753\n",
            "epochs =  53 , step =  300 , loss_val =  0.34664166\n",
            "epochs =  53 , step =  400 , loss_val =  0.27560735\n",
            "epochs =  53 , step =  500 , loss_val =  0.43798733\n",
            "epochs =  54 , step =  0 , loss_val =  0.5203677\n",
            "epochs =  54 , step =  100 , loss_val =  0.23611905\n",
            "epochs =  54 , step =  200 , loss_val =  0.38711575\n",
            "epochs =  54 , step =  300 , loss_val =  0.3553843\n",
            "epochs =  54 , step =  400 , loss_val =  0.35923344\n",
            "epochs =  54 , step =  500 , loss_val =  0.3599992\n",
            "epochs =  55 , step =  0 , loss_val =  0.4130489\n",
            "epochs =  55 , step =  100 , loss_val =  0.2939765\n",
            "epochs =  55 , step =  200 , loss_val =  0.46871975\n",
            "epochs =  55 , step =  300 , loss_val =  0.3762012\n",
            "epochs =  55 , step =  400 , loss_val =  0.4415835\n",
            "epochs =  55 , step =  500 , loss_val =  0.4442131\n",
            "epochs =  56 , step =  0 , loss_val =  0.14620529\n",
            "epochs =  56 , step =  100 , loss_val =  0.32293403\n",
            "epochs =  56 , step =  200 , loss_val =  0.36890993\n",
            "epochs =  56 , step =  300 , loss_val =  0.46445972\n",
            "epochs =  56 , step =  400 , loss_val =  0.26224416\n",
            "epochs =  56 , step =  500 , loss_val =  0.3066168\n",
            "epochs =  57 , step =  0 , loss_val =  0.25638598\n",
            "epochs =  57 , step =  100 , loss_val =  0.41724288\n",
            "epochs =  57 , step =  200 , loss_val =  0.2839261\n",
            "epochs =  57 , step =  300 , loss_val =  0.41712424\n",
            "epochs =  57 , step =  400 , loss_val =  0.33910483\n",
            "epochs =  57 , step =  500 , loss_val =  0.22559716\n",
            "epochs =  58 , step =  0 , loss_val =  0.48862523\n",
            "epochs =  58 , step =  100 , loss_val =  0.35622212\n",
            "epochs =  58 , step =  200 , loss_val =  0.2839112\n",
            "epochs =  58 , step =  300 , loss_val =  0.458597\n",
            "epochs =  58 , step =  400 , loss_val =  0.21891682\n",
            "epochs =  58 , step =  500 , loss_val =  0.39748627\n",
            "epochs =  59 , step =  0 , loss_val =  0.3391491\n",
            "epochs =  59 , step =  100 , loss_val =  0.5068341\n",
            "epochs =  59 , step =  200 , loss_val =  0.40222085\n",
            "epochs =  59 , step =  300 , loss_val =  0.33623862\n",
            "epochs =  59 , step =  400 , loss_val =  0.29476887\n",
            "epochs =  59 , step =  500 , loss_val =  0.28261158\n",
            "epochs =  60 , step =  0 , loss_val =  0.44437012\n",
            "epochs =  60 , step =  100 , loss_val =  0.24700265\n",
            "epochs =  60 , step =  200 , loss_val =  0.3870058\n",
            "epochs =  60 , step =  300 , loss_val =  0.4746862\n",
            "epochs =  60 , step =  400 , loss_val =  0.25910512\n",
            "epochs =  60 , step =  500 , loss_val =  0.34842056\n",
            "epochs =  61 , step =  0 , loss_val =  0.35767052\n",
            "epochs =  61 , step =  100 , loss_val =  0.503617\n",
            "epochs =  61 , step =  200 , loss_val =  0.38401383\n",
            "epochs =  61 , step =  300 , loss_val =  0.45843843\n",
            "epochs =  61 , step =  400 , loss_val =  0.44068238\n",
            "epochs =  61 , step =  500 , loss_val =  0.47860852\n",
            "epochs =  62 , step =  0 , loss_val =  0.251508\n",
            "epochs =  62 , step =  100 , loss_val =  0.28987157\n",
            "epochs =  62 , step =  200 , loss_val =  0.33908063\n",
            "epochs =  62 , step =  300 , loss_val =  0.36091134\n",
            "epochs =  62 , step =  400 , loss_val =  0.22860508\n",
            "epochs =  62 , step =  500 , loss_val =  0.47219703\n",
            "epochs =  63 , step =  0 , loss_val =  0.37463927\n",
            "epochs =  63 , step =  100 , loss_val =  0.27000338\n",
            "epochs =  63 , step =  200 , loss_val =  0.37675205\n",
            "epochs =  63 , step =  300 , loss_val =  0.3999039\n",
            "epochs =  63 , step =  400 , loss_val =  0.42281002\n",
            "epochs =  63 , step =  500 , loss_val =  0.303726\n",
            "epochs =  64 , step =  0 , loss_val =  0.48027444\n",
            "epochs =  64 , step =  100 , loss_val =  0.58669966\n",
            "epochs =  64 , step =  200 , loss_val =  0.21780321\n",
            "epochs =  64 , step =  300 , loss_val =  0.31685156\n",
            "epochs =  64 , step =  400 , loss_val =  0.31172696\n",
            "epochs =  64 , step =  500 , loss_val =  0.33175918\n",
            "epochs =  65 , step =  0 , loss_val =  0.45015502\n",
            "epochs =  65 , step =  100 , loss_val =  0.30843058\n",
            "epochs =  65 , step =  200 , loss_val =  0.40470898\n",
            "epochs =  65 , step =  300 , loss_val =  0.27981228\n",
            "epochs =  65 , step =  400 , loss_val =  0.16152157\n",
            "epochs =  65 , step =  500 , loss_val =  0.26820153\n",
            "epochs =  66 , step =  0 , loss_val =  0.25810328\n",
            "epochs =  66 , step =  100 , loss_val =  0.39268672\n",
            "epochs =  66 , step =  200 , loss_val =  0.29125395\n",
            "epochs =  66 , step =  300 , loss_val =  0.19923571\n",
            "epochs =  66 , step =  400 , loss_val =  0.3896641\n",
            "epochs =  66 , step =  500 , loss_val =  0.40956512\n",
            "epochs =  67 , step =  0 , loss_val =  0.27547643\n",
            "epochs =  67 , step =  100 , loss_val =  0.25664565\n",
            "epochs =  67 , step =  200 , loss_val =  0.29856652\n",
            "epochs =  67 , step =  300 , loss_val =  0.41205516\n",
            "epochs =  67 , step =  400 , loss_val =  0.35871306\n",
            "epochs =  67 , step =  500 , loss_val =  0.44224918\n",
            "epochs =  68 , step =  0 , loss_val =  0.26617974\n",
            "epochs =  68 , step =  100 , loss_val =  0.35972172\n",
            "epochs =  68 , step =  200 , loss_val =  0.4374596\n",
            "epochs =  68 , step =  300 , loss_val =  0.5026934\n",
            "epochs =  68 , step =  400 , loss_val =  0.28595755\n",
            "epochs =  68 , step =  500 , loss_val =  0.29805163\n",
            "epochs =  69 , step =  0 , loss_val =  0.32106143\n",
            "epochs =  69 , step =  100 , loss_val =  0.36161077\n",
            "epochs =  69 , step =  200 , loss_val =  0.2478179\n",
            "epochs =  69 , step =  300 , loss_val =  0.115092814\n",
            "epochs =  69 , step =  400 , loss_val =  0.5135963\n",
            "epochs =  69 , step =  500 , loss_val =  0.38901293\n",
            "epochs =  70 , step =  0 , loss_val =  0.5500738\n",
            "epochs =  70 , step =  100 , loss_val =  0.35132062\n",
            "epochs =  70 , step =  200 , loss_val =  0.2592384\n",
            "epochs =  70 , step =  300 , loss_val =  0.23207414\n",
            "epochs =  70 , step =  400 , loss_val =  0.29476982\n",
            "epochs =  70 , step =  500 , loss_val =  0.21579449\n",
            "epochs =  71 , step =  0 , loss_val =  0.3368343\n",
            "epochs =  71 , step =  100 , loss_val =  0.36462253\n",
            "epochs =  71 , step =  200 , loss_val =  0.3354879\n",
            "epochs =  71 , step =  300 , loss_val =  0.2404564\n",
            "epochs =  71 , step =  400 , loss_val =  0.41959772\n",
            "epochs =  71 , step =  500 , loss_val =  0.26497784\n",
            "epochs =  72 , step =  0 , loss_val =  0.3313308\n",
            "epochs =  72 , step =  100 , loss_val =  0.29214883\n",
            "epochs =  72 , step =  200 , loss_val =  0.37809122\n",
            "epochs =  72 , step =  300 , loss_val =  0.22437887\n",
            "epochs =  72 , step =  400 , loss_val =  0.26395813\n",
            "epochs =  72 , step =  500 , loss_val =  0.3572377\n",
            "epochs =  73 , step =  0 , loss_val =  0.29173538\n",
            "epochs =  73 , step =  100 , loss_val =  0.34137997\n",
            "epochs =  73 , step =  200 , loss_val =  0.27607182\n",
            "epochs =  73 , step =  300 , loss_val =  0.21524858\n",
            "epochs =  73 , step =  400 , loss_val =  0.38396725\n",
            "epochs =  73 , step =  500 , loss_val =  0.38388813\n",
            "epochs =  74 , step =  0 , loss_val =  0.4082988\n",
            "epochs =  74 , step =  100 , loss_val =  0.43555966\n",
            "epochs =  74 , step =  200 , loss_val =  0.28378063\n",
            "epochs =  74 , step =  300 , loss_val =  0.40311658\n",
            "epochs =  74 , step =  400 , loss_val =  0.39011756\n",
            "epochs =  74 , step =  500 , loss_val =  0.20696002\n",
            "epochs =  75 , step =  0 , loss_val =  0.45622796\n",
            "epochs =  75 , step =  100 , loss_val =  0.37420738\n",
            "epochs =  75 , step =  200 , loss_val =  0.43872818\n",
            "epochs =  75 , step =  300 , loss_val =  0.21429075\n",
            "epochs =  75 , step =  400 , loss_val =  0.36019066\n",
            "epochs =  75 , step =  500 , loss_val =  0.3828058\n",
            "epochs =  76 , step =  0 , loss_val =  0.1791159\n",
            "epochs =  76 , step =  100 , loss_val =  0.2030821\n",
            "epochs =  76 , step =  200 , loss_val =  0.1879225\n",
            "epochs =  76 , step =  300 , loss_val =  0.27113324\n",
            "epochs =  76 , step =  400 , loss_val =  0.24571683\n",
            "epochs =  76 , step =  500 , loss_val =  0.22144292\n",
            "epochs =  77 , step =  0 , loss_val =  0.3549744\n",
            "epochs =  77 , step =  100 , loss_val =  0.37327603\n",
            "epochs =  77 , step =  200 , loss_val =  0.2149268\n",
            "epochs =  77 , step =  300 , loss_val =  0.24287157\n",
            "epochs =  77 , step =  400 , loss_val =  0.3148542\n",
            "epochs =  77 , step =  500 , loss_val =  0.23243286\n",
            "epochs =  78 , step =  0 , loss_val =  0.44593814\n",
            "epochs =  78 , step =  100 , loss_val =  0.4115553\n",
            "epochs =  78 , step =  200 , loss_val =  0.21154432\n",
            "epochs =  78 , step =  300 , loss_val =  0.32554024\n",
            "epochs =  78 , step =  400 , loss_val =  0.42680776\n",
            "epochs =  78 , step =  500 , loss_val =  0.18584831\n",
            "epochs =  79 , step =  0 , loss_val =  0.38025534\n",
            "epochs =  79 , step =  100 , loss_val =  0.20424205\n",
            "epochs =  79 , step =  200 , loss_val =  0.38981843\n",
            "epochs =  79 , step =  300 , loss_val =  0.2993345\n",
            "epochs =  79 , step =  400 , loss_val =  0.30820006\n",
            "epochs =  79 , step =  500 , loss_val =  0.38507423\n",
            "epochs =  80 , step =  0 , loss_val =  0.26028946\n",
            "epochs =  80 , step =  100 , loss_val =  0.17381066\n",
            "epochs =  80 , step =  200 , loss_val =  0.375083\n",
            "epochs =  80 , step =  300 , loss_val =  0.47479165\n",
            "epochs =  80 , step =  400 , loss_val =  0.40947327\n",
            "epochs =  80 , step =  500 , loss_val =  0.28903073\n",
            "epochs =  81 , step =  0 , loss_val =  0.20161812\n",
            "epochs =  81 , step =  100 , loss_val =  0.3795618\n",
            "epochs =  81 , step =  200 , loss_val =  0.2532103\n",
            "epochs =  81 , step =  300 , loss_val =  0.18605809\n",
            "epochs =  81 , step =  400 , loss_val =  0.3123187\n",
            "epochs =  81 , step =  500 , loss_val =  0.324883\n",
            "epochs =  82 , step =  0 , loss_val =  0.3621951\n",
            "epochs =  82 , step =  100 , loss_val =  0.5251108\n",
            "epochs =  82 , step =  200 , loss_val =  0.20681502\n",
            "epochs =  82 , step =  300 , loss_val =  0.25192234\n",
            "epochs =  82 , step =  400 , loss_val =  0.18618858\n",
            "epochs =  82 , step =  500 , loss_val =  0.26374897\n",
            "epochs =  83 , step =  0 , loss_val =  0.31885105\n",
            "epochs =  83 , step =  100 , loss_val =  0.30044872\n",
            "epochs =  83 , step =  200 , loss_val =  0.35634384\n",
            "epochs =  83 , step =  300 , loss_val =  0.30668935\n",
            "epochs =  83 , step =  400 , loss_val =  0.4905654\n",
            "epochs =  83 , step =  500 , loss_val =  0.31438312\n",
            "epochs =  84 , step =  0 , loss_val =  0.19344014\n",
            "epochs =  84 , step =  100 , loss_val =  0.42591614\n",
            "epochs =  84 , step =  200 , loss_val =  0.23952389\n",
            "epochs =  84 , step =  300 , loss_val =  0.3752441\n",
            "epochs =  84 , step =  400 , loss_val =  0.30353206\n",
            "epochs =  84 , step =  500 , loss_val =  0.15610012\n",
            "epochs =  85 , step =  0 , loss_val =  0.24708012\n",
            "epochs =  85 , step =  100 , loss_val =  0.44393903\n",
            "epochs =  85 , step =  200 , loss_val =  0.63318443\n",
            "epochs =  85 , step =  300 , loss_val =  0.38277692\n",
            "epochs =  85 , step =  400 , loss_val =  0.26273102\n",
            "epochs =  85 , step =  500 , loss_val =  0.3219092\n",
            "epochs =  86 , step =  0 , loss_val =  0.245506\n",
            "epochs =  86 , step =  100 , loss_val =  0.2941747\n",
            "epochs =  86 , step =  200 , loss_val =  0.40672305\n",
            "epochs =  86 , step =  300 , loss_val =  0.22763304\n",
            "epochs =  86 , step =  400 , loss_val =  0.3355761\n",
            "epochs =  86 , step =  500 , loss_val =  0.09893144\n",
            "epochs =  87 , step =  0 , loss_val =  0.29047284\n",
            "epochs =  87 , step =  100 , loss_val =  0.31416574\n",
            "epochs =  87 , step =  200 , loss_val =  0.14600319\n",
            "epochs =  87 , step =  300 , loss_val =  0.14202116\n",
            "epochs =  87 , step =  400 , loss_val =  0.25300744\n",
            "epochs =  87 , step =  500 , loss_val =  0.35140756\n",
            "epochs =  88 , step =  0 , loss_val =  0.3826606\n",
            "epochs =  88 , step =  100 , loss_val =  0.43495515\n",
            "epochs =  88 , step =  200 , loss_val =  0.35042217\n",
            "epochs =  88 , step =  300 , loss_val =  0.23031491\n",
            "epochs =  88 , step =  400 , loss_val =  0.26853332\n",
            "epochs =  88 , step =  500 , loss_val =  0.26568335\n",
            "epochs =  89 , step =  0 , loss_val =  0.41616112\n",
            "epochs =  89 , step =  100 , loss_val =  0.33699834\n",
            "epochs =  89 , step =  200 , loss_val =  0.35153148\n",
            "epochs =  89 , step =  300 , loss_val =  0.42158207\n",
            "epochs =  89 , step =  400 , loss_val =  0.31848115\n",
            "epochs =  89 , step =  500 , loss_val =  0.38371286\n",
            "epochs =  90 , step =  0 , loss_val =  0.28084725\n",
            "epochs =  90 , step =  100 , loss_val =  0.36876705\n",
            "epochs =  90 , step =  200 , loss_val =  0.31925365\n",
            "epochs =  90 , step =  300 , loss_val =  0.33090106\n",
            "epochs =  90 , step =  400 , loss_val =  0.29952666\n",
            "epochs =  90 , step =  500 , loss_val =  0.5070761\n",
            "epochs =  91 , step =  0 , loss_val =  0.37948352\n",
            "epochs =  91 , step =  100 , loss_val =  0.4994253\n",
            "epochs =  91 , step =  200 , loss_val =  0.21575081\n",
            "epochs =  91 , step =  300 , loss_val =  0.3984977\n",
            "epochs =  91 , step =  400 , loss_val =  0.26496553\n",
            "epochs =  91 , step =  500 , loss_val =  0.5920185\n",
            "epochs =  92 , step =  0 , loss_val =  0.49433857\n",
            "epochs =  92 , step =  100 , loss_val =  0.27292228\n",
            "epochs =  92 , step =  200 , loss_val =  0.19711953\n",
            "epochs =  92 , step =  300 , loss_val =  0.368819\n",
            "epochs =  92 , step =  400 , loss_val =  0.3856739\n",
            "epochs =  92 , step =  500 , loss_val =  0.27156076\n",
            "epochs =  93 , step =  0 , loss_val =  0.24709226\n",
            "epochs =  93 , step =  100 , loss_val =  0.24538192\n",
            "epochs =  93 , step =  200 , loss_val =  0.2684299\n",
            "epochs =  93 , step =  300 , loss_val =  0.32299164\n",
            "epochs =  93 , step =  400 , loss_val =  0.3273472\n",
            "epochs =  93 , step =  500 , loss_val =  0.38077793\n",
            "epochs =  94 , step =  0 , loss_val =  0.5850183\n",
            "epochs =  94 , step =  100 , loss_val =  0.34973046\n",
            "epochs =  94 , step =  200 , loss_val =  0.32550216\n",
            "epochs =  94 , step =  300 , loss_val =  0.23579095\n",
            "epochs =  94 , step =  400 , loss_val =  0.2978468\n",
            "epochs =  94 , step =  500 , loss_val =  0.27029645\n",
            "epochs =  95 , step =  0 , loss_val =  0.427538\n",
            "epochs =  95 , step =  100 , loss_val =  0.2771603\n",
            "epochs =  95 , step =  200 , loss_val =  0.22551279\n",
            "epochs =  95 , step =  300 , loss_val =  0.23868366\n",
            "epochs =  95 , step =  400 , loss_val =  0.12403286\n",
            "epochs =  95 , step =  500 , loss_val =  0.3721151\n",
            "epochs =  96 , step =  0 , loss_val =  0.32534963\n",
            "epochs =  96 , step =  100 , loss_val =  0.22972278\n",
            "epochs =  96 , step =  200 , loss_val =  0.21337385\n",
            "epochs =  96 , step =  300 , loss_val =  0.333296\n",
            "epochs =  96 , step =  400 , loss_val =  0.30554476\n",
            "epochs =  96 , step =  500 , loss_val =  0.32909194\n",
            "epochs =  97 , step =  0 , loss_val =  0.22318192\n",
            "epochs =  97 , step =  100 , loss_val =  0.23961712\n",
            "epochs =  97 , step =  200 , loss_val =  0.29807067\n",
            "epochs =  97 , step =  300 , loss_val =  0.22556514\n",
            "epochs =  97 , step =  400 , loss_val =  0.3126127\n",
            "epochs =  97 , step =  500 , loss_val =  0.3710482\n",
            "epochs =  98 , step =  0 , loss_val =  0.27735478\n",
            "epochs =  98 , step =  100 , loss_val =  0.36290288\n",
            "epochs =  98 , step =  200 , loss_val =  0.40559873\n",
            "epochs =  98 , step =  300 , loss_val =  0.21658432\n",
            "epochs =  98 , step =  400 , loss_val =  0.24268778\n",
            "epochs =  98 , step =  500 , loss_val =  0.32022426\n",
            "epochs =  99 , step =  0 , loss_val =  0.24719268\n",
            "epochs =  99 , step =  100 , loss_val =  0.33600858\n",
            "epochs =  99 , step =  200 , loss_val =  0.23178959\n",
            "epochs =  99 , step =  300 , loss_val =  0.25720298\n",
            "epochs =  99 , step =  400 , loss_val =  0.3215103\n",
            "epochs =  99 , step =  500 , loss_val =  0.29844445\n",
            "\n",
            "Elapsed Time =>  0:01:17.278287\n",
            "\n",
            "\n",
            "Accuracy =  0.9132\n",
            "type(accuracy_val) =  <class 'numpy.float32'> , type(predicted_list_val) =  <class 'numpy.ndarray'> , type(index_label) =  <class 'numpy.ndarray'>\n",
            "index_label.shape =  (10000,)\n",
            "length of index_label_list =  10000\n",
            "false label count =  868\n",
            "\n",
            "length of index_label_prediction_list 868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pkJ6t2wGsNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "2b578a66-c060-4f5e-b2b9-34f4a329c59c"
      },
      "source": [
        "print(index_label_prediction_list)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8, 5, 6], [18, 3, 2], [33, 4, 0], [38, 2, 3], [46, 1, 3], [63, 3, 2], [66, 6, 2], [73, 9, 7], [92, 9, 4], [119, 2, 8], [124, 7, 4], [149, 2, 8], [151, 9, 3], [172, 2, 3], [175, 7, 3], [187, 5, 3], [193, 9, 4], [195, 3, 5], [211, 5, 9], [217, 6, 5], [233, 8, 3], [241, 9, 8], [245, 3, 5], [247, 4, 6], [259, 6, 0], [264, 9, 3], [299, 8, 3], [300, 4, 6], [320, 9, 1], [325, 4, 9], [340, 5, 3], [341, 6, 4], [352, 5, 0], [358, 7, 9], [386, 6, 5], [425, 4, 6], [435, 8, 7], [444, 2, 8], [445, 6, 0], [448, 9, 5], [478, 5, 8], [479, 9, 5], [488, 9, 7], [495, 8, 2], [502, 5, 1], [507, 3, 5], [511, 4, 5], [516, 2, 6], [528, 3, 2], [531, 3, 6], [536, 2, 1], [543, 8, 3], [551, 7, 1], [553, 8, 6], [565, 4, 9], [578, 3, 8], [582, 8, 2], [591, 8, 3], [610, 4, 2], [611, 0, 6], [613, 2, 8], [619, 1, 8], [628, 3, 9], [629, 2, 6], [638, 5, 0], [659, 2, 8], [667, 7, 8], [691, 8, 4], [692, 5, 9], [707, 4, 9], [717, 0, 6], [720, 5, 8], [726, 7, 9], [740, 4, 9], [741, 2, 8], [787, 8, 9], [791, 5, 9], [797, 5, 8], [810, 7, 2], [823, 2, 8], [829, 4, 5], [830, 2, 3], [839, 8, 3], [841, 7, 2], [857, 5, 3], [866, 5, 6], [877, 8, 6], [881, 4, 9], [882, 9, 7], [898, 7, 2], [900, 1, 3], [924, 2, 7], [939, 2, 0], [947, 8, 9], [956, 1, 6], [958, 3, 0], [959, 4, 3], [965, 6, 0], [982, 3, 8], [999, 9, 7], [1014, 6, 5], [1017, 6, 2], [1024, 4, 7], [1032, 5, 6], [1039, 7, 8], [1050, 2, 5], [1062, 3, 7], [1082, 5, 3], [1089, 5, 0], [1101, 8, 2], [1107, 9, 3], [1112, 4, 6], [1114, 3, 8], [1124, 8, 2], [1128, 3, 2], [1135, 5, 1], [1147, 4, 7], [1152, 9, 7], [1159, 4, 6], [1178, 4, 6], [1181, 6, 1], [1191, 0, 4], [1198, 8, 5], [1200, 8, 5], [1204, 3, 7], [1206, 7, 2], [1226, 7, 2], [1228, 9, 3], [1232, 9, 6], [1234, 8, 3], [1242, 4, 9], [1247, 9, 0], [1253, 4, 6], [1256, 2, 3], [1263, 4, 7], [1269, 2, 6], [1283, 7, 2], [1289, 5, 9], [1291, 3, 5], [1299, 5, 7], [1319, 8, 3], [1325, 8, 6], [1326, 7, 2], [1328, 7, 9], [1331, 5, 3], [1337, 2, 6], [1357, 4, 2], [1364, 8, 2], [1391, 4, 9], [1393, 5, 3], [1410, 2, 6], [1413, 4, 8], [1421, 5, 0], [1422, 4, 9], [1429, 9, 4], [1440, 4, 9], [1444, 6, 2], [1446, 2, 3], [1465, 4, 6], [1467, 5, 9], [1494, 7, 0], [1500, 7, 1], [1514, 2, 8], [1522, 7, 9], [1525, 5, 0], [1527, 1, 8], [1529, 4, 6], [1530, 8, 7], [1549, 4, 6], [1553, 9, 5], [1559, 9, 3], [1569, 6, 2], [1581, 7, 9], [1584, 8, 3], [1601, 3, 2], [1609, 2, 4], [1621, 0, 6], [1626, 6, 5], [1634, 4, 7], [1640, 9, 2], [1641, 5, 0], [1671, 7, 3], [1681, 3, 7], [1709, 9, 5], [1717, 8, 0], [1718, 7, 9], [1720, 4, 6], [1721, 7, 9], [1722, 2, 6], [1727, 3, 7], [1732, 9, 5], [1737, 5, 2], [1740, 8, 2], [1748, 0, 6], [1751, 4, 2], [1754, 7, 2], [1759, 8, 6], [1772, 7, 4], [1773, 1, 5], [1790, 2, 7], [1798, 3, 5], [1800, 6, 4], [1813, 8, 5], [1819, 6, 0], [1828, 3, 7], [1842, 2, 8], [1843, 0, 2], [1850, 8, 7], [1855, 8, 3], [1878, 8, 3], [1883, 7, 9], [1901, 9, 4], [1903, 7, 4], [1917, 5, 8], [1930, 2, 6], [1938, 4, 2], [1952, 9, 5], [1955, 8, 2], [1957, 2, 6], [1968, 8, 1], [1969, 6, 2], [1970, 5, 3], [1973, 8, 3], [1982, 6, 5], [1984, 2, 0], [2016, 7, 2], [2024, 7, 9], [2035, 5, 3], [2040, 5, 6], [2043, 4, 8], [2044, 2, 7], [2053, 4, 9], [2063, 7, 1], [2068, 9, 4], [2073, 5, 0], [2098, 2, 0], [2109, 3, 7], [2110, 2, 8], [2118, 6, 5], [2129, 9, 2], [2130, 4, 9], [2135, 6, 1], [2145, 4, 2], [2148, 4, 9], [2183, 4, 6], [2186, 2, 3], [2189, 9, 8], [2192, 5, 3], [2197, 4, 6], [2224, 5, 8], [2237, 5, 6], [2266, 1, 8], [2279, 5, 3], [2293, 9, 0], [2305, 3, 8], [2325, 7, 2], [2347, 3, 5], [2351, 3, 2], [2369, 5, 8], [2370, 0, 6], [2371, 4, 9], [2374, 2, 8], [2380, 9, 0], [2387, 9, 1], [2393, 8, 3], [2395, 8, 2], [2396, 2, 0], [2400, 5, 8], [2404, 4, 2], [2406, 9, 8], [2408, 3, 8], [2422, 6, 4], [2425, 8, 1], [2433, 2, 1], [2447, 4, 9], [2460, 5, 8], [2462, 2, 6], [2475, 2, 8], [2488, 2, 6], [2496, 2, 3], [2515, 5, 6], [2526, 5, 3], [2534, 3, 5], [2542, 6, 5], [2545, 5, 3], [2548, 9, 4], [2556, 5, 8], [2559, 5, 3], [2560, 3, 2], [2573, 5, 1], [2574, 5, 7], [2582, 9, 7], [2598, 8, 2], [2607, 7, 2], [2611, 5, 6], [2630, 4, 9], [2631, 0, 6], [2635, 2, 7], [2648, 9, 0], [2654, 6, 1], [2658, 4, 6], [2695, 7, 4], [2697, 5, 0], [2713, 0, 8], [2739, 8, 1], [2743, 5, 8], [2751, 6, 0], [2770, 3, 5], [2771, 4, 9], [2802, 2, 6], [2832, 5, 3], [2836, 4, 3], [2853, 3, 8], [2863, 9, 4], [2896, 8, 0], [2900, 4, 2], [2906, 3, 5], [2907, 4, 9], [2919, 5, 8], [2921, 3, 2], [2927, 3, 2], [2939, 9, 7], [2946, 1, 8], [2953, 3, 5], [2963, 4, 7], [2970, 5, 7], [2972, 0, 2], [2977, 2, 3], [2986, 5, 6], [2995, 6, 8], [3026, 1, 2], [3030, 6, 8], [3056, 9, 7], [3060, 9, 3], [3062, 8, 5], [3073, 1, 3], [3078, 3, 8], [3100, 5, 3], [3102, 5, 3], [3114, 4, 6], [3117, 5, 9], [3130, 6, 0], [3136, 7, 9], [3138, 3, 0], [3145, 5, 9], [3158, 8, 6], [3160, 9, 4], [3164, 6, 2], [3186, 8, 6], [3189, 7, 4], [3193, 3, 2], [3222, 6, 2], [3225, 7, 1], [3240, 9, 3], [3269, 6, 2], [3289, 8, 5], [3297, 0, 6], [3316, 7, 4], [3329, 7, 2], [3336, 5, 7], [3369, 9, 1], [3376, 7, 9], [3377, 4, 6], [3381, 3, 2], [3384, 2, 6], [3405, 4, 9], [3414, 5, 3], [3422, 6, 0], [3437, 4, 9], [3441, 7, 2], [3450, 0, 8], [3456, 3, 8], [3468, 5, 6], [3490, 4, 3], [3502, 8, 6], [3503, 9, 5], [3514, 2, 1], [3520, 6, 4], [3525, 7, 2], [3549, 3, 2], [3550, 6, 5], [3558, 5, 0], [3567, 8, 5], [3573, 7, 4], [3597, 9, 3], [3598, 1, 5], [3604, 7, 0], [3612, 4, 9], [3645, 5, 0], [3654, 5, 8], [3674, 8, 3], [3681, 2, 3], [3685, 4, 5], [3687, 9, 4], [3688, 6, 8], [3702, 5, 3], [3718, 4, 9], [3727, 8, 6], [3751, 7, 8], [3756, 5, 8], [3757, 8, 2], [3763, 5, 7], [3767, 7, 2], [3776, 5, 8], [3778, 5, 0], [3780, 4, 2], [3782, 8, 2], [3787, 3, 8], [3796, 2, 1], [3806, 5, 8], [3808, 7, 8], [3811, 2, 3], [3817, 2, 5], [3818, 0, 2], [3820, 9, 7], [3821, 9, 4], [3832, 2, 7], [3833, 8, 2], [3834, 3, 2], [3838, 7, 1], [3839, 2, 1], [3841, 4, 2], [3853, 6, 5], [3855, 5, 0], [3862, 2, 3], [3869, 9, 4], [3876, 2, 8], [3893, 5, 2], [3902, 5, 3], [3906, 1, 3], [3918, 5, 0], [3926, 9, 3], [3941, 4, 2], [3942, 0, 6], [3943, 3, 5], [3946, 2, 8], [3951, 8, 3], [3968, 5, 3], [3975, 3, 2], [3976, 7, 3], [3984, 9, 8], [3985, 9, 4], [3994, 5, 0], [3998, 4, 6], [4000, 9, 3], [4007, 7, 4], [4017, 4, 9], [4027, 7, 9], [4031, 5, 3], [4044, 3, 5], [4052, 5, 3], [4059, 5, 8], [4063, 6, 5], [4065, 0, 2], [4068, 8, 3], [4072, 5, 3], [4075, 8, 0], [4076, 5, 8], [4078, 9, 2], [4093, 9, 4], [4102, 7, 9], [4131, 5, 3], [4140, 8, 2], [4145, 8, 3], [4152, 5, 1], [4154, 9, 4], [4159, 8, 3], [4163, 9, 0], [4176, 2, 4], [4177, 5, 6], [4197, 4, 6], [4201, 1, 9], [4207, 8, 6], [4211, 6, 5], [4212, 1, 3], [4224, 9, 7], [4238, 7, 3], [4248, 2, 8], [4256, 3, 2], [4259, 9, 4], [4263, 5, 3], [4265, 4, 8], [4271, 5, 3], [4289, 2, 7], [4291, 4, 9], [4294, 9, 7], [4297, 7, 2], [4300, 5, 8], [4306, 3, 7], [4313, 4, 9], [4315, 5, 8], [4326, 2, 6], [4355, 5, 3], [4359, 5, 7], [4369, 9, 4], [4374, 5, 6], [4423, 7, 9], [4433, 7, 3], [4435, 3, 7], [4451, 2, 1], [4455, 8, 6], [4463, 5, 6], [4487, 7, 2], [4497, 8, 7], [4498, 7, 9], [4507, 1, 2], [4523, 8, 2], [4548, 5, 0], [4575, 4, 2], [4578, 7, 9], [4601, 8, 4], [4615, 2, 4], [4639, 8, 3], [4640, 8, 3], [4657, 3, 2], [4690, 7, 2], [4692, 9, 3], [4693, 7, 9], [4723, 2, 7], [4731, 8, 7], [4735, 9, 4], [4740, 3, 9], [4761, 9, 7], [4785, 3, 8], [4807, 8, 3], [4823, 9, 4], [4826, 4, 2], [4827, 4, 9], [4833, 3, 2], [4837, 7, 2], [4839, 8, 2], [4852, 8, 6], [4874, 9, 0], [4879, 8, 5], [4880, 0, 6], [4882, 6, 8], [4886, 7, 1], [4890, 8, 6], [4915, 5, 8], [4918, 9, 4], [4950, 2, 3], [4956, 8, 4], [4963, 9, 7], [4966, 7, 8], [4987, 4, 2], [4990, 3, 8], [4998, 4, 9], [5038, 3, 2], [5054, 3, 8], [5067, 3, 2], [5068, 4, 1], [5078, 3, 2], [5135, 9, 4], [5140, 3, 2], [5143, 3, 2], [5165, 0, 3], [5209, 8, 6], [5217, 2, 8], [5288, 8, 5], [5298, 8, 5], [5331, 1, 6], [5403, 8, 9], [5409, 4, 6], [5449, 2, 8], [5457, 1, 8], [5573, 3, 5], [5593, 0, 6], [5600, 7, 9], [5601, 8, 6], [5611, 8, 1], [5613, 0, 6], [5617, 4, 9], [5623, 3, 0], [5642, 1, 5], [5643, 2, 6], [5650, 8, 5], [5653, 0, 6], [5676, 4, 3], [5678, 8, 6], [5687, 3, 8], [5714, 7, 9], [5717, 2, 8], [5734, 3, 7], [5746, 1, 8], [5749, 8, 6], [5780, 3, 2], [5821, 5, 3], [5841, 3, 2], [5842, 4, 7], [5854, 7, 9], [5858, 7, 9], [5862, 5, 3], [5866, 7, 9], [5874, 5, 3], [5887, 7, 0], [5888, 4, 0], [5891, 5, 2], [5912, 3, 0], [5913, 5, 3], [5922, 5, 3], [5926, 4, 9], [5936, 4, 9], [5945, 3, 8], [5949, 7, 9], [5955, 3, 8], [5957, 5, 8], [5973, 3, 8], [5975, 4, 9], [5992, 7, 9], [6019, 4, 9], [6026, 7, 8], [6035, 2, 0], [6037, 4, 9], [6042, 5, 3], [6043, 5, 3], [6059, 3, 8], [6071, 9, 3], [6080, 8, 6], [6081, 9, 5], [6091, 9, 5], [6157, 9, 0], [6160, 3, 8], [6166, 9, 3], [6168, 9, 3], [6172, 9, 5], [6173, 9, 6], [6227, 5, 3], [6324, 5, 6], [6347, 8, 6], [6384, 2, 6], [6385, 5, 3], [6386, 5, 0], [6390, 5, 8], [6391, 2, 6], [6400, 0, 6], [6421, 3, 2], [6426, 0, 6], [6455, 3, 5], [6480, 2, 0], [6501, 3, 5], [6505, 9, 0], [6511, 3, 5], [6517, 3, 0], [6528, 2, 6], [6553, 4, 6], [6555, 8, 9], [6558, 6, 2], [6560, 9, 3], [6564, 3, 9], [6566, 2, 6], [6568, 9, 4], [6569, 3, 2], [6571, 9, 7], [6572, 1, 8], [6575, 3, 5], [6576, 7, 2], [6577, 7, 8], [6597, 0, 7], [6598, 5, 2], [6603, 8, 5], [6610, 9, 4], [6625, 8, 7], [6632, 9, 5], [6636, 3, 8], [6641, 8, 5], [6651, 0, 5], [6706, 5, 3], [6721, 2, 6], [6735, 8, 5], [6739, 3, 2], [6740, 9, 5], [6744, 2, 8], [6746, 5, 4], [6765, 8, 5], [6768, 7, 2], [6785, 2, 6], [6872, 4, 6], [6878, 2, 6], [6906, 2, 6], [6964, 5, 3], [6981, 5, 6], [7002, 2, 8], [7015, 2, 6], [7043, 9, 7], [7049, 0, 6], [7081, 9, 7], [7094, 8, 9], [7121, 8, 9], [7130, 3, 8], [7178, 5, 0], [7198, 8, 3], [7208, 8, 2], [7216, 0, 2], [7233, 3, 5], [7248, 3, 8], [7258, 6, 5], [7338, 4, 8], [7391, 4, 9], [7402, 4, 8], [7426, 9, 4], [7432, 7, 1], [7434, 4, 8], [7446, 0, 6], [7451, 5, 8], [7459, 9, 5], [7472, 2, 4], [7491, 0, 5], [7492, 2, 7], [7494, 4, 5], [7498, 5, 3], [7539, 2, 8], [7558, 0, 2], [7580, 9, 4], [7587, 2, 1], [7691, 8, 3], [7713, 8, 5], [7779, 5, 8], [7789, 2, 3], [7797, 5, 6], [7800, 3, 2], [7809, 5, 8], [7821, 3, 2], [7847, 1, 2], [7849, 3, 2], [7851, 6, 0], [7856, 1, 2], [7858, 3, 2], [7860, 6, 0], [7870, 5, 4], [7875, 3, 2], [7886, 2, 4], [7888, 5, 4], [7899, 1, 8], [7902, 7, 9], [7905, 3, 2], [7915, 7, 9], [7916, 1, 2], [7920, 1, 2], [7921, 8, 6], [7945, 2, 6], [7958, 5, 0], [8010, 3, 2], [8020, 1, 8], [8047, 2, 6], [8050, 3, 2], [8062, 5, 8], [8072, 5, 3], [8081, 4, 6], [8091, 2, 8], [8092, 4, 6], [8094, 2, 8], [8095, 4, 6], [8245, 2, 3], [8246, 3, 5], [8253, 2, 4], [8259, 9, 5], [8273, 0, 7], [8311, 6, 2], [8322, 9, 6], [8325, 0, 2], [8332, 9, 7], [8339, 8, 6], [8358, 9, 4], [8362, 3, 5], [8394, 2, 3], [8406, 4, 9], [8408, 8, 6], [8416, 4, 8], [8442, 4, 6], [8453, 5, 3], [8504, 9, 5], [8520, 4, 9], [8522, 8, 6], [8639, 2, 3], [8863, 5, 6], [8984, 7, 9], [9007, 3, 8], [9009, 7, 2], [9010, 2, 8], [9015, 7, 2], [9016, 0, 5], [9019, 7, 2], [9022, 3, 2], [9024, 7, 2], [9025, 1, 8], [9036, 7, 2], [9045, 7, 3], [9046, 2, 8], [9057, 9, 5], [9071, 1, 8], [9103, 4, 6], [9182, 3, 8], [9209, 2, 8], [9280, 8, 5], [9398, 5, 3], [9433, 8, 6], [9446, 2, 6], [9465, 5, 3], [9482, 5, 0], [9487, 2, 6], [9538, 4, 8], [9544, 9, 7], [9560, 7, 2], [9587, 9, 4], [9595, 2, 5], [9612, 1, 7], [9614, 3, 5], [9624, 3, 8], [9634, 0, 2], [9642, 9, 7], [9643, 1, 7], [9645, 1, 7], [9653, 1, 7], [9655, 3, 2], [9664, 2, 7], [9666, 0, 3], [9679, 6, 5], [9698, 6, 5], [9699, 1, 2], [9701, 9, 7], [9706, 2, 0], [9716, 2, 0], [9726, 2, 0], [9729, 5, 6], [9733, 9, 8], [9738, 4, 6], [9741, 9, 7], [9742, 3, 2], [9744, 8, 1], [9745, 4, 2], [9749, 5, 6], [9751, 2, 0], [9752, 2, 0], [9755, 8, 5], [9764, 4, 8], [9768, 2, 0], [9770, 5, 0], [9777, 5, 0], [9779, 2, 0], [9781, 7, 9], [9783, 4, 2], [9792, 4, 9], [9808, 9, 4], [9811, 2, 8], [9814, 5, 8], [9839, 2, 7], [9840, 3, 2], [9853, 5, 1], [9856, 9, 4], [9858, 6, 8], [9862, 6, 8], [9867, 2, 8], [9879, 0, 2], [9881, 3, 5], [9888, 6, 2], [9893, 2, 8], [9905, 3, 7], [9913, 2, 3], [9925, 3, 2], [9926, 8, 1], [9940, 6, 0], [9944, 3, 8], [9959, 8, 7], [9962, 0, 5], [9970, 5, 3], [9975, 3, 2], [9982, 5, 2], [9986, 3, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brVtxEWDGsMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "afcb6d7a-5a78-43a2-a458-96166e551cb2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "random_index = int(np.random.choice(len(index_label_prediction_list), 1))\n",
        "\n",
        "index_str = 'index = ' + str(index_label_prediction_list[random_index][0]) + ', '\n",
        "label_str = 'label = ' + str(index_label_prediction_list[random_index][1]) + ', '\n",
        "prediction_str = 'prediction = ' + str(index_label_prediction_list[random_index][2]) + ', '\n",
        "\n",
        "title_str = index_str + label_str + prediction_str\n",
        "\n",
        "img = test_x_data[index_label_prediction_list[random_index][0]].reshape(28,28)\n",
        "\n",
        "plt.title(title_str)\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAEICAYAAACj9mr/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAU+0lEQVR4nO3de7CcdX3H8fcHgkEuFgIkhBgSQGpIrIKTQTugpINyUwv0EkWBIEiwCEWHVjG9yNgotgNEOlUwFgQK4WK5WhnkYp1oGSiHy5CQFCV4IPfIJRAQCoRv//j9jn2y7G/3ZHfP2c3h85o5c559fvs8z3d/++xnf8+zz56jiMDMrJ6tul2AmfUuB4SZFTkgzKzIAWFmRQ4IMytyQJhZUVsBIelRSTNaXPZySXPb2f5IJ2mypJA0ahD3nSFpRYvbaXnZLZWkfkkfydNzJP1ri+tp+TWwJWgrICJiWkT8rEO1dI2kqZL6JD2Xf+6SNLXS/iVJT0h6QdIqSfMGXrSS9pT0Ys1PSDq7svxukhZIej6v/+puPM7hJOlDhX75027XVisivhkRn2t2v3pvar32GpC0n6Sf5n3tcUnHtrM+H2Ikq4A/A8YAuwK3AtdW2m8F3h8R7wDeA7wP+EuAiHgqInYY+AH+AHgDuKGy/I3AGmBPYCxw/tA+nO6LiJ/X9MvHgReB2zu9rcGMsN4Kcj/cAvwHaV+eDVwl6fdbXWe7hxjVYdq5kq6XdKWkDXnoNb1y3wMkPZjbrgO2rVnXxyU9LGm9pHskvTfP/6SkX0t6R759pKQ1knZrp/aqiFgfEf2RLisVsBF4V6V9WUSsHyiVFADvevOaADgRWBgR/bnew4CJwF9HxPMR8VpEPNRKnZI+K2lp7sMnJJ1W5z5zJD2dn5vPVOaPlnS+pKckrZV0iaS3t1JHi2YB/x4RLw3mzrn+r0pakkddP5C0bW6bIWmFpK9IWgP8QNJWks6RtEzSM3lfHFNZ3wmSnsxtf1OzrXMlXVW5fXDeB9dLWi7pJEmzgc8AX86joR9V6hx4DYyW9O08ylyVp0fX1Hy2pHWSVkv6bHtd+iZTgD2AeRGxMSJ+CvwXcELLa4yIln+AfuAjefpc4BXgKGBr4Dzg3tz2NuBJ4EvANqR369eAubn9AGAd8IG87Ky87tG5/WrgcmAX0rv9xxvUtL7BzzlNHs964HVSAPxtTdungReAAH4DvK/O8gKWASdV5v098BPgKuAZ4H7gkEH27+S8vVH59seAffJ2DgF+SxrZAMzItV8IjM7tLwHvzu3zSCOhMcCOwI+A8yrLrmhQxyMN+vS7g3gc2wMbgBmbuW8tJoXrGNKOPrfmsf5jfqxvB84C7gXemed9D7gm338qafTy4dx2YV6+uu9elacn5VqPI+2ruwD757bLB2oovAa+nmsYC+wG3AP8Q03NX8/rPSo/fzsXHv93G/T5I4Vl3pMfpyrz7gRuavk13uGAuKvSNhV4OU9/mPTCrhZ+T+UJv3igIyvtj5FfSMBOwFPAIuB77dQ8yJ35dOBjhfZ9gX8Adq/T9qH8BO1QmTef9CI/Je8Yn8pP8q6DqGUylYCo034zcFbNDrh9pf164O9IgfISsE+l7Q+BX1eWLQZEB/r0BODX1ed/kPvW5yu3jwKWVep9Fdi20r4UOLRyezzpTWgUKaSvrXmOX6V+QHy19IKieUAsA46qtB0O9Fdqfrn6XJLeFD/YwX7eBngC+HKePiw/zp+0us5On4NYU5n+LbBtPi7aA1gZ+VFkT1amJwFn5yHdeknrSe8ce0A6BAB+SErICzpc8yYiDYEvAa6UNLZO+6+AR0kJX2sWcENEvFiZ9zJpJ7k00uHFtcBy4KDNrS0fXt0r6dncR0eRzpkMeC42HcI/SerD3YDtgAcq/Xt7nj8cZgFX1jz/g7G8Mj3wWAb8JiJeqdyeBNxUeXxLSYeK4/Jyv1tX7qNnCtucSHqht2IPNt2va2t+JiJer9z+LbBDi9t6k4h4DTiGNNJcA5xNepNo+ROq4TpJuRqYIEmVeXtWppcD34iInSo/20XENQCS9gdOBq4B/rnRhuqcOa/+zBlkvVuRXlATCu2jSEP96nbfDvw5cEXNfR8hjQKqNvsrtPlY9gbSCc5xEbETcBtpdDBgZ0nbV27vSRq5PU0KqmmV/v29SCcPB7PtRxv06SVNlp1Ieve8crCPtWJinccyoLYPlwNH1uxD20bEStL+97t1SdqOdOhQz3JqntsG26y1ihRUpZoHLZ8jKvX5o6XlIuKRiDgkInaJiMOBvYH/bqWGgRW2M6Tpp84wLWqGx6RzEE+RjhO3Af6ETc9BTCc9MR8g7fDbk1JwR9LJzMXAX5COHxcBp3dqWJa3/1HSeZCtgXeQQmgVeQgLfA4Ym6enkkYQF9as49O5P1QzfwzwHOlddGvS+ZdnyYcYud9+Vqir2oc7kt4RD8l9dCTpHaj2uPz83N8fIh1WTMntF5HeTQYexwTg8MqyQ3KIAcwhnbStnT8j7X4N961FpHMKY4BfAN8s1Us6v/UzYFK+vRtwdJ6eRjr0Ozj3zfmUz0HsSToHMTP3e/UcxLeABQ1eA3NJh867kUZ2v6h5fmpr/t2yHezv95JeM9sBf0U6tBtdaQ8241zQsIwgIuJVUiicRHpxfJL00d9Aex9wKvAvpBfT4/m+kE52Lo+IiyPif4HjgbmS9u1giTuRRifPk4aX+wBHxP8PYQ8CFkl6ifSufRtpx6+aBfxb5Geh8tieBf6Y9GQ9D5xD2nGfzneZSDoB11BEbCB9tHo9qY8+TTrpWLUmt60indj9fET8T277Cqlf75X0AnAX8O5m2+2AE3nzqArS476nybILgDtIx9XLSC/AkotI/XGHpA2kk4UfAIiIR4Ev5PWtJvVR3WF3RDxFOnQ7m7SvPkz6WBvgUmBqPoy5uc7ic4E+0qhxEfBgk5qHwgmkx7gOOBT4aH7dDIzmNuTaBkU1+7MNM0kPk06ulY6JRySlKxd/GBE/KbT3A5+LiLuGtbARTNLxpMPMrw56GQeE9SIHRG/wlZRmVuQRhJkVeQRhZkXD+iUXSR6umA2xiFDzew1Ou1/WOkLSY0pfKz2nU0WZWW9o+RyEpK2BX5IuMlpB+hLScRGxpMEyHkGYDbFeGUEcCDweEU/kC6GuBY7uTFlm1gvaCYgJbPplmhXU+e6CpNlKf62pr41tmVkXDPlJyoiYT/rKsw8xzLYw7YwgVrLpt+3emeeZ2QjRTkDcD+wraS9JbyP9IZTaLw+Z2Ras5UOMiHhd0hmkP6e2NXBZ/tacmY0Qw3qptc9BmA29XvmY08xGOAeEmRU5IMysyAFhZkUOCDMrckCYWZEDwsyKHBBmVuSAMLMiB4SZFTkgzKzIAWFmRQ4IMytyQJhZkQPCzIocEGZW5IAwsyIHhJkVOSDMrMgBYWZFDggzK3JAmFmRA8LMihwQZlbkgDCzIgeEmRU5IMysyAFhZkUOCDMrckCYWdGodhaW1A9sADYCr0fE9E4UZWa9oa2AyP4oIp7uwHrMrMf4EMPMitoNiADukPSApNn17iBptqQ+SX1tbsvMhpkiovWFpQkRsVLSWOBO4MyIWNjg/q1vzMwGJSLUqXW1NYKIiJX59zrgJuDAThRlZr2h5YCQtL2kHQemgcOAxZ0qzMy6r51PMcYBN0kaWM+CiLi9I1XZJsaOHduw/e677y62PfbYYw2XXbZsWUs1Dejra3xq6bXXXiu27bfffg2XPe+881qqyTqn5YCIiCeA93WwFjPrMf6Y08yKHBBmVuSAMLMiB4SZFTkgzKyorSspN3tjvpKyrlGjGn+Y9NBDDzVsnzZtWifLGTbLly9v2D5p0qRhqmRk6ZkrKc1sZHNAmFmRA8LMihwQZlbkgDCzIgeEmRU5IMysqBN/tNbatNVWjXN6S73OwbZ8HkGYWZEDwsyKHBBmVuSAMLMiB4SZFTkgzKzIAWFmRb4Oogfsvffe3S7BrC6PIMysyAFhZkUOCDMrckCYWZEDwsyKHBBmVuSAMLMiXwfRA2bOnDlk6z7rrLMati9cuLBh+ymnnNKw/dRTT23YPnr06GLbkiVLGi5r3dd0BCHpMknrJC2uzBsj6U5Jv8q/dx7aMs2sGwZziHE5cETNvHOAuyNiX+DufNvMRpimARERC4Fna2YfDVyRp68AjulwXWbWA1o9BzEuIlbn6TXAuNIdJc0GZre4HTProrZPUkZENPqnvBExH5gP/ue9ZluaVj/mXCtpPED+va5zJZlZr2g1IG4FZuXpWcAtnSnHzHqJIhqP+iVdA8wAdgXWAl8DbgauB/YEngRmRkTticx66/IhRh2rVq1q2L777rs3bL/uuuuKbSeffHLDZV9++eWG7c2sWbOmYfvYsWOLbWeeeWbDZb/zne+0VNNbXUSoU+tqeg4iIo4rNB3aqSLMrDf5UmszK3JAmFmRA8LMihwQZlbkgDCzIn/dexicdtppDdubfYzZzO23315sa/djzOOPP75he6OPMW3L5xGEmRU5IMysyAFhZkUOCDMrckCYWZEDwsyKHBBmVuTrIIbB4Ycf3tbyGzZsaNh+//33t7X+RkaN8i7yVuYRhJkVOSDMrMgBYWZFDggzK3JAmFmRA8LMihwQZlbkD7mHweTJk9ta/pJLLmnYvmTJkrbW38guu+wyZOu23ucRhJkVOSDMrMgBYWZFDggzK3JAmFmRA8LMihwQZlakiBi+jUnDt7Ee0uw6iGnTpjVs//GPf9zBajZPs2sspkyZ0rD9lVdeKbZNnTq14bL9/f0N262+iFCn1tV0BCHpMknrJC2uzDtX0kpJD+efozpVkJn1jsEcYlwOHFFn/ryI2D//3NbZssysFzQNiIhYCDw7DLWYWY9p5yTlGZIeyYcgO5fuJGm2pD5JfW1sy8y6oNWAuBjYB9gfWA1cULpjRMyPiOkRMb3FbZlZl7QUEBGxNiI2RsQbwPeBAztblpn1gpYCQtL4ys1jgcWl+5rZlqvp34OQdA0wA9hV0grga8AMSfsDAfQDpw1hjVu8Zp/nj+TP+zdu3FhsG8mPe6RoGhARcVyd2ZcOQS1m1mN8qbWZFTkgzKzIAWFmRQ4IMytyQJhZkQPCzIocEGZW5IAwsyIHhJkVOSDMrMgBYWZFDggzK3JAmFmRA8LMihwQZlbkgDCzIgeEmRU5IMysyAFhZkUOCDMrckCYWVHTv2ptI9snPvGJhu1Tpkxpa/2vvvpqW8tbd3kEYWZFDggzK3JAmFmRA8LMihwQZlbkgDCzIgeEmRU1vQ5C0kTgSmAcEMD8iLhI0hjgOmAy0A/MjIjnhq5UGwqS2mqPiIbt8+bN2+yarHcMZgTxOnB2REwFPgh8QdJU4Bzg7ojYF7g73zazEaRpQETE6oh4ME9vAJYCE4CjgSvy3a4AjhmqIs2sOzbrHISkycABwH3AuIhYnZvWkA5BzGwEGfR3MSTtANwAfDEiXqgem0ZESKp7MCppNjC73ULNbPgNagQhaRtSOFwdETfm2Wsljc/t44F19ZaNiPkRMT0ipneiYDMbPk0DQmmocCmwNCIurDTdCszK07OAWzpfnpl102AOMQ4CTgAWSXo4z5sDfAu4XtIpwJPAzKEp0bqp2ceYNrI1DYiI+AVQ+jD80M6WY2a9xFdSmlmRA8LMihwQZlbkgDCzIgeEmRU5IMysyH/23obUzJnly2Pmzp07jJVYKzyCMLMiB4SZFTkgzKzIAWFmRQ4IMytyQJhZkQPCzIp8HcRb3Iknnjik699rr72GdP02tDyCMLMiB4SZFTkgzKzIAWFmRQ4IMytyQJhZkQPCzIp8HcRb3LhxQ/svVU8//fQhXb8NLY8gzKzIAWFmRQ4IMytyQJhZkQPCzIocEGZW5IAws6Km10FImghcCYwDApgfERdJOhc4FfhNvuuciLhtqAq1oXHfffc1bD/ooIMatvf19TVsX7BgwWbXZL1jMBdKvQ6cHREPStoReEDSnbltXkScP3TlmVk3NQ2IiFgNrM7TGyQtBSYMdWFm1n2bdQ5C0mTgAGBgXHqGpEckXSZp58IysyX1SWo8FjWznjPogJC0A3AD8MWIeAG4GNgH2J80wrig3nIRMT8ipkfE9A7Ua2bDaFABIWkbUjhcHRE3AkTE2ojYGBFvAN8HDhy6Ms2sG5oGhCQBlwJLI+LCyvzxlbsdCyzufHlm1k2KiMZ3kA4Gfg4sAt7Is+cAx5EOLwLoB07LJzQbravxxsysbRGhTq2raUB0kgPCbOh1MiB8JaWZFTkgzKzIAWFmRQ4IMytyQJhZkQPCzIocEGZW5IAwsyIHhJkVOSDMrMgBYWZFDggzK3JAmFmRA8LMigbzV6076WngycrtXfO8XtSrtfVqXeDaWtXJ2iZ1aD3AMP89iDdtXOrr1b9V2au19Wpd4Npa1cu1+RDDzIocEGZW1O2AmN/l7TfSq7X1al3g2lrVs7V19RyEmfW2bo8gzKyHOSDMrKgrASHpCEmPSXpc0jndqKFEUr+kRZIe7vb/E83/83SdpMWVeWMk3SnpV/l33f+J2qXazpW0Mvfdw5KO6lJtEyX9p6Qlkh6VdFae39W+a1BXT/RbPcN+DkLS1sAvgY8CK4D7geMiYsmwFlIgqR+YHhFdv6hG0oeBF4ErI+I9ed4/Ac9GxLdyuO4cEV/pkdrOBV6MiPOHu56a2sYD4yPiQUk7Ag8AxwAn0cW+a1DXTHqg3+rpxgjiQODxiHgiIl4FrgWO7kIdPS8iFgLP1sw+GrgiT19B2sGGXaG2nhARqyPiwTy9AVgKTKDLfdegrp7VjYCYACyv3F5Bb3VSAHdIekDS7G4XU8e4yr84XAOM62YxdZwh6ZF8CNKVw58qSZOBA4D76KG+q6kLeqzfBvgk5ZsdHBHvB44EvpCH0j0p0vFhL31OfTGwD+l/tq4GLuhmMZJ2IP1X+i9GxAvVtm72XZ26eqrfqroRECuBiZXb78zzekJErMy/1wE3kQ6Jesnagf+snn+v63I9vxMRayNiY0S8AXyfLvadpG1IL8KrI+LGPLvrfVevrl7qt1rdCIj7gX0l7SXpbcCngFu7UMebSNo+nzxC0vbAYcDixksNu1uBWXl6FnBLF2vZxMCLLzuWLvWdJAGXAksj4sJKU1f7rlRXr/RbPV25kjJ/jPNtYGvgsoj4xrAXUYekvUmjBkhfhV/QzdokXQPMIH0deC3wNeBm4HpgT9JX52dGxLCfLCzUNoM0TA6gHzitcsw/nLUdDPwcWAS8kWfPIR3vd63vGtR1HD3Qb/X4UmszK/JJSjMrckCYWZEDwsyKHBBmVuSAMLMiB4SZFTkgzKzo/wC6+0bqSrwh+QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvf2FDPfGsIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3zYa6EmGsFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFxbi6ulGrW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNyOODl4GrpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6RNKr9UGrjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ee3U8xVGrg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg24KU_DGrch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}